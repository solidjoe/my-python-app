{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNtY/GT6sdzgan6gV5ez7Ge",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solidjoe/my-python-app/blob/main/Hindi_Factivity_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "INPUT_FILE = '/content/Hindi_Experiment_Data.xlsx'\n",
        "OUTPUT_FILE = 'Hindi_Transformed_Data_Final.xlsx'\n",
        "\n",
        "def process_hindi_experiment(input_path, output_path):\n",
        "    if not os.path.exists(input_path):\n",
        "        print(f\"‚ùå Error: {input_path} not found.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Reading file: {input_path}...\")\n",
        "\n",
        "    # 1. LOAD DATA\n",
        "    if input_path.endswith('.csv'):\n",
        "        try:\n",
        "            df = pd.read_csv(input_path, encoding='utf-8')\n",
        "        except UnicodeDecodeError:\n",
        "            df = pd.read_csv(input_path, encoding='latin1')\n",
        "    else:\n",
        "\n",
        "        df = pd.read_excel(input_path)\n",
        "\n",
        "    # 2. FILTERING: Keep only complete sessions (12 rows per block)\n",
        "    session_groups = ['Participant_ID', 'Task_Block']\n",
        "    counts = df.groupby(session_groups).size().reset_index(name='Row_Count')\n",
        "    valid_sessions = counts[counts['Row_Count'] == 12]\n",
        "\n",
        "    df_filtered = pd.merge(df, valid_sessions[session_groups], on=session_groups, how='inner')\n",
        "\n",
        "    # 3. TRANSFORMATIONS\n",
        "    df_filtered['Rating_Normalized'] = df_filtered['Rating'] / 100.0\n",
        "    N = len(df_filtered)\n",
        "    df_filtered['Rating_Beta'] = (df_filtered['Rating_Normalized'] * (N - 1) + 0.5) / N\n",
        "\n",
        "    # 4. FORMATTING\n",
        "    target_cols = ['Timestamp', 'Participant_ID', 'Verb', 'Verb_Type',\n",
        "                   'Task_Block', 'Rating', 'Rating_Normalized', 'Rating_Beta']\n",
        "    final_df = df_filtered[[col for col in target_cols if col in df_filtered.columns]]\n",
        "\n",
        "    # 5. OUTPUT AS EXCEL\n",
        "\n",
        "    final_df.to_excel(output_path, index=False)\n",
        "\n",
        "    print(f\"‚úÖ Success! Excel file saved as: {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_hindi_experiment(INPUT_FILE, OUTPUT_FILE)"
      ],
      "metadata": {
        "id": "xAwi-L7UJ51n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. LOAD THE DATA\n",
        "\n",
        "FILE_PATH = '/content/Hindi_Transformed_Data_Final.xlsx'\n",
        "\n",
        "try:\n",
        "\n",
        "    df = pd.read_excel(FILE_PATH)\n",
        "    print(\"‚úÖ File loaded successfully.\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: Could not find or read the file. {e}\")\n",
        "\n",
        "    raise\n",
        "\n",
        "#  2. CALCULATE SD PER SESSION\n",
        "# This calculates the Standard Deviation of the 'Rating' for each block (12 items)\n",
        "session_variances = df.groupby(['Participant_ID', 'Task_Block'])['Rating'].std().reset_index()\n",
        "session_variances.columns = ['Participant_ID', 'Task_Block', 'Response_SD']\n",
        "\n",
        "#  3. CALCULATE GROUP METRICS\n",
        "# We find the average 'volatility' of the whole group\n",
        "group_mean = session_variances['Response_SD'].mean()\n",
        "group_std  = session_variances['Response_SD'].std()\n",
        "\n",
        "# 4. DEFINE THE ZOMBIE THRESHOLD\n",
        "# Logic: 3 Standard Deviations below the mean (searching for 'flatliners')\n",
        "threshold = group_mean - (3 * group_std)\n",
        "\n",
        "print(f\"üìä Average Participant SD: {group_mean:.2f}\")\n",
        "print(f\"üìä Group SD of Variances: {group_std:.2f}\")\n",
        "print(f\"üö´ Zombie Threshold (3SD below mean): {threshold:.2f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "#  5. IDENTIFY THE ZOMBIES\n",
        "zombies = session_variances[session_variances['Response_SD'] < threshold]\n",
        "\n",
        "if len(zombies) > 0:\n",
        "    print(f\"üßü Found {len(zombies)} Zombie Session(s):\")\n",
        "    print(zombies)\n",
        "else:\n",
        "    print(\"‚úÖ No Zombies found! All participants moved the sliders sufficiently.\")\n",
        "\n",
        "# Optional: Show the top 5 most 'consistent' (potential low variance) participants\n",
        "print(\"\\nüîç Most 'Flat' Responders (Lowest SD):\")\n",
        "print(session_variances.sort_values(by='Response_SD').head())"
      ],
      "metadata": {
        "id": "A6x_DENCekCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# 1. Load Data\n",
        "df = pd.read_csv('Hindi_Transformed_Data_1.csv')\n",
        "\n",
        "# 2. Individual Trial Correlation (276 pairs)\n",
        "df_paired = df.pivot_table(index=['Participant_ID', 'Verb'],\n",
        "                          columns='Task_Block',\n",
        "                          values='Rating_Normalized').dropna()\n",
        "\n",
        "r_trial, p_trial = pearsonr(df_paired['At-issueness'], df_paired['Projection'])\n",
        "\n",
        "# 3. Aggregated Verb Correlation (12 verbs)\n",
        "verb_means = df.groupby(['Verb', 'Task_Block'])['Rating_Normalized'].mean().unstack()\n",
        "\n",
        "r_agg, p_agg = pearsonr(verb_means['At-issueness'], verb_means['Projection'])\n",
        "\n",
        "# --- ADD THESE PRINT STATEMENTS TO SEE THE OUTPUT ---\n",
        "print(f\"--- Correlation Results ---\")\n",
        "print(f\"Individual Trial Correlation (n={len(df_paired)} pairs):\")\n",
        "print(f\"  Pearson's r: {r_trial:.4f}\")\n",
        "print(f\"  p-value:     {p_trial:.4f}\")\n",
        "\n",
        "print(f\"\\nAggregated Verb Correlation (n={len(verb_means)} verbs):\")\n",
        "print(f\"  Pearson's r: {r_agg:.4f}\")\n",
        "print(f\"  p-value:     {p_agg:.4f}\")"
      ],
      "metadata": {
        "id": "I5B78nUC44GE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Load your cleaned dataset\n",
        "df = pd.read_csv('Hindi_Transformed_Data_1.csv')\n",
        "\n",
        "# 2. Calculate the means per Verb and Task_Block\n",
        "# We use Rating_Normalized to get the 0.0 - 1.0 scale\n",
        "verb_means = df.groupby(['Verb', 'Task_Block'])['Rating_Normalized'].mean().unstack().reset_index()\n",
        "\n",
        "# Rename columns to match your requested format\n",
        "verb_means.columns = ['Hindi Verb', 'Mean At-issueness', 'Mean Projection']\n",
        "\n",
        "# 3. Pull the Verb Type from the original data\n",
        "verb_info = df[['Verb', 'Verb_Type']].drop_duplicates().rename(columns={'Verb': 'Hindi Verb', 'Verb_Type': 'Verb Type'})\n",
        "final_table = pd.merge(verb_means, verb_info, on='Hindi Verb')\n",
        "\n",
        "# 4. Define English Translations\n",
        "translation_dict = {\n",
        "    '‡§¨‡§§‡§æ‡§®‡§æ': 'tell',\n",
        "    '‡§ú‡§æ‡§®‡§®‡§æ': 'know',\n",
        "    '‡§Ø‡§æ‡§¶ ‡§π‡•ã‡§®‡§æ': 'remember',\n",
        "    '‡§™‡§§‡§æ ‡§ö‡§≤‡§®‡§æ': 'find out',\n",
        "    '‡§∏‡•Å‡§®‡§®‡§æ': 'hear',\n",
        "    '‡§ï‡§π‡§®‡§æ': 'say',\n",
        "    '‡§¶‡•á‡§ñ‡§®‡§æ': 'see',\n",
        "    '‡§Æ‡§æ‡§®‡§®‡§æ': 'believe',\n",
        "    '‡§™‡•Ç‡§õ‡§®‡§æ': 'ask',\n",
        "    '‡§≤‡§ó‡§®‡§æ': 'seem',\n",
        "    '‡§∏‡•ã‡§ö‡§®‡§æ': 'think',\n",
        "    '‡§∏‡§Æ‡§ù‡§®‡§æ': 'understand'\n",
        "}\n",
        "final_table['English'] = final_table['Hindi Verb'].map(translation_dict)\n",
        "\n",
        "# 5. Reorder columns and sort by Projection (highest to lowest)\n",
        "final_table = final_table[['Hindi Verb', 'English', 'Verb Type', 'Mean Projection', 'Mean At-issueness']]\n",
        "final_table = final_table.sort_values(by='Mean Projection', ascending=False)\n",
        "\n",
        "# 6. Print the formatted table\n",
        "print(final_table.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "hFjZbIQAFQRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# 1. Create the data\n",
        "data = {\n",
        "    'Hindi Verb': ['‡§¨‡§§‡§æ‡§®‡§æ', '‡§ú‡§æ‡§®‡§®‡§æ', '‡§Ø‡§æ‡§¶ ‡§π‡•ã‡§®‡§æ', '‡§™‡§§‡§æ ‡§ö‡§≤‡§®‡§æ', '‡§∏‡•Å‡§®‡§®‡§æ', '‡§ï‡§π‡§®‡§æ', '‡§¶‡•á‡§ñ‡§®‡§æ', '‡§Æ‡§æ‡§®‡§®‡§æ', '‡§™‡•Ç‡§õ‡§®‡§æ', '‡§≤‡§ó‡§®‡§æ', '‡§∏‡•ã‡§ö‡§®‡§æ', '‡§∏‡§Æ‡§ù‡§®‡§æ'],\n",
        "    'English': ['tell', 'know', 'remember', 'find out', 'hear', 'say', 'see', 'believe', 'ask', 'seem', 'think', 'understand'],\n",
        "    'Verb Type': ['non-factive', 'factive', 'factive', 'factive', 'factive', 'non-factive', 'factive', 'non-factive', 'non-factive', 'non-factive', 'non-factive', 'factive'],\n",
        "    'Mean Projection': [0.758, 0.757, 0.742, 0.699, 0.625, 0.615, 0.606, 0.576, 0.495, 0.451, 0.440, 0.392],\n",
        "    'Mean At-issueness': [0.592, 0.442, 0.487, 0.624, 0.563, 0.629, 0.450, 0.525, 0.528, 0.484, 0.388, 0.372]\n",
        "}\n",
        "\n",
        "df_plot = pd.DataFrame(data)\n",
        "\n",
        "# 2. Calculate Correlation for the Title\n",
        "r, p = pearsonr(df_plot['Mean At-issueness'], df_plot['Mean Projection'])\n",
        "\n",
        "# 3. Setup the Plot\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot with a regression line\n",
        "ax = sns.regplot(data=df_plot, x='Mean At-issueness', y='Mean Projection',\n",
        "                 scatter_kws={'s': 100, 'alpha': 0.6}, line_kws={\"color\": \"red\"})\n",
        "\n",
        "# 4. Add labels for each point\n",
        "for i in range(df_plot.shape[0]):\n",
        "    plt.text(df_plot['Mean At-issueness'][i] + 0.005,\n",
        "             df_plot['Mean Projection'][i] + 0.005,\n",
        "             df_plot['English'][i],\n",
        "             fontsize=11, weight='semibold')\n",
        "\n",
        "# 5. Customize Axes and Title\n",
        "plt.title(f'Hindi Lexical Correlation: At-issueness vs Projection\\n(Pearson r = {r:.2f}, p = {p:.3f})', fontsize=14)\n",
        "plt.xlabel('Mean At-issueness (Is the main point p?)', fontsize=12)\n",
        "plt.ylabel('Mean Projection (Are you certain that p?)', fontsize=12)\n",
        "plt.xlim(0.3, 0.8) # Adjusting limits to center the data\n",
        "plt.ylim(0.3, 0.8)\n",
        "\n",
        "# Add a diagonal line to show where \"English GPP\" would expect the data (top-left)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qUDfxFZ6JLVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# 1. Input the summary table data\n",
        "data = {\n",
        "    'Hindi Verb': ['‡§¨‡§§‡§æ‡§®‡§æ', '‡§ú‡§æ‡§®‡§®‡§æ', '‡§Ø‡§æ‡§¶ ‡§π‡•ã‡§®‡§æ', '‡§™‡§§‡§æ ‡§ö‡§≤‡§®‡§æ', '‡§∏‡•Å‡§®‡§®‡§æ', '‡§ï‡§π‡§®‡§æ', '‡§¶‡•á‡§ñ‡§®‡§æ', '‡§Æ‡§æ‡§®‡§®‡§æ', '‡§™‡•Ç‡§õ‡§®‡§æ', '‡§≤‡§ó‡§®‡§æ', '‡§∏‡•ã‡§ö‡§®‡§æ', '‡§∏‡§Æ‡§ù‡§®‡§æ'],\n",
        "    'English': ['tell', 'know', 'remember', 'find out', 'hear', 'say', 'see', 'believe', 'ask', 'seem', 'think', 'understand'],\n",
        "    'Verb Type': ['non-factive', 'factive', 'factive', 'factive', 'factive', 'non-factive', 'factive', 'non-factive', 'non-factive', 'non-factive', 'non-factive', 'factive'],\n",
        "    'Mean Projection': [0.758, 0.757, 0.742, 0.699, 0.625, 0.615, 0.606, 0.576, 0.495, 0.451, 0.440, 0.392],\n",
        "    'Mean At-issueness': [0.592, 0.442, 0.487, 0.624, 0.563, 0.629, 0.450, 0.525, 0.528, 0.484, 0.388, 0.372]\n",
        "}\n",
        "\n",
        "df_plot = pd.DataFrame(data)\n",
        "\n",
        "# 2. Calculate Pearson Correlation for the Title\n",
        "r, p = pearsonr(df_plot['Mean At-issueness'], df_plot['Mean Projection'])\n",
        "\n",
        "# 3. Create the Visualization\n",
        "plt.figure(figsize=(11, 8))\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Draw the regression line (overall trend)\n",
        "sns.regplot(data=df_plot, x='Mean At-issueness', y='Mean Projection',\n",
        "            scatter=False, color='gray', line_kws={\"linestyle\": \"--\", \"alpha\": 0.5})\n",
        "\n",
        "# Draw the scatter points color-coded by Verb Type\n",
        "sns.scatterplot(data=df_plot, x='Mean At-issueness', y='Mean Projection',\n",
        "                hue='Verb Type', style='Verb Type', s=150, palette='Set1', edgecolor='black')\n",
        "\n",
        "# 4. Annotate each point with its English translation\n",
        "for i in range(df_plot.shape[0]):\n",
        "    plt.text(df_plot['Mean At-issueness'][i] + 0.008,\n",
        "             df_plot['Mean Projection'][i] + 0.008,\n",
        "             df_plot['English'][i],\n",
        "             fontsize=10, fontweight='bold', alpha=0.8)\n",
        "\n",
        "# 5. Formatting the Plot\n",
        "plt.title(f'Hindi Lexical Correlation: At-issueness vs Projection\\n(r = {r:.2f}, p = {p:.3f})',\n",
        "          fontsize=15, pad=20, fontweight='bold')\n",
        "plt.xlabel('Mean At-issueness (Is the main point p?)', fontsize=12)\n",
        "plt.ylabel('Mean Projection (Are you certain that p?)', fontsize=12)\n",
        "\n",
        "# Set limits to show the 0-1 scale clearly, zoomed into the data range\n",
        "plt.xlim(0.3, 0.7)\n",
        "plt.ylim(0.3, 0.8)\n",
        "\n",
        "plt.legend(title='Verb Category', loc='upper left')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZGER-pUvL41J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bambi arviz"
      ],
      "metadata": {
        "id": "HpgacTr5kqNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import bambi as bmb\n",
        "import arviz as az\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load the data\n",
        "df = pd.read_csv('Hindi_Transformed_Data_1.csv')\n",
        "\n",
        "# 2. Prepare the \"Wide\" format\n",
        "# To see if At-issueness predicts Projection, they must be in the same row\n",
        "df_wide = df.pivot_table(index=['Participant_ID', 'Verb', 'Verb_Type'],\n",
        "                          columns='Task_Block',\n",
        "                          values='Rating_Beta').reset_index()\n",
        "\n",
        "# Clean up column names (removing spaces/dashes for the formula)\n",
        "df_wide.columns = ['Participant_ID', 'Verb', 'Verb_Type', 'At_issueness', 'Projection']\n",
        "\n",
        "# 3. Define the Bayesian Mixed-Effects Beta Regression\n",
        "# Formula: Projection is predicted by At-issueness.\n",
        "# (1|Participant_ID): Each person gets their own baseline.\n",
        "# (1|Verb): Each verb gets its own baseline.\n",
        "model = bmb.Model(\n",
        "    \"Projection ~ At_issueness + (1|Participant_ID) + (1|Verb)\",\n",
        "    df_wide,\n",
        "    family=\"beta\"\n",
        ")\n",
        "\n",
        "# 4. Fit the model (This uses MCMC sampling)\n",
        "# draws=2000: The number of \"simulations\" the AI runs to find the truth.\n",
        "results = model.fit(draws=2000, tune=1000, target_accept=0.9)\n",
        "\n",
        "# 5. Output the Results\n",
        "print(\"--- Bayesian Model Summary ---\")\n",
        "summary = az.summary(results, var_names=[\"At_issueness\"])\n",
        "print(summary)\n",
        "\n",
        "# 6. Visualize the \"Posterior\" (The probability distribution of your result)\n",
        "az.plot_posterior(results, var_names=[\"At_issueness\"])\n",
        "plt.title(\"Probability Distribution of the At-issueness Effect\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J0x4jWpjkwBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "INPUT_FILE = '/content/Hindi_Transformed_Data_2.csv'  # Your source file\n",
        "OUTPUT_FILE = 'Hindi_Transformed_Data_1.csv' # Your target output file\n",
        "\n",
        "def transform_ratings(input_path, output_path):\n",
        "    # 1. Load the Data\n",
        "    if not os.path.exists(input_path):\n",
        "        print(f\"‚ùå Error: Input file not found at {input_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Loading data from {input_path}...\")\n",
        "    # Supports CSV or Excel\n",
        "    if input_path.endswith('.xlsx'):\n",
        "        df = pd.read_excel(input_path)\n",
        "    else:\n",
        "        df = pd.read_csv(input_path)\n",
        "\n",
        "    # 2. Verify Sample Size (N)\n",
        "\n",
        "    N = len(df)\n",
        "    print(f\"Dataset Size (N): {N}\")\n",
        "\n",
        "    # 3. Calculate Normalized Rating (0 to 1)\n",
        "\n",
        "    df['Rating_Normalized'] = df['Rating'] / 100.0\n",
        "\n",
        "    # 4. Calculate Beta Rating (Smithson & Verkuilen, 2006)\n",
        "    # Formula: y' = [ y*(N-1) + 0.5 ] / N\n",
        "    df['Rating_Beta'] = (df['Rating_Normalized'] * (N - 1) + 0.5) / N\n",
        "\n",
        "    # 5. Format and Save\n",
        "\n",
        "    columns_to_keep = [\n",
        "        'Timestamp', 'Participant_ID', 'Verb', 'Verb_Type',\n",
        "        'Task_Block', 'Rating', 'Rating_Normalized', 'Rating_Beta'\n",
        "    ]\n",
        "\n",
        "    # Check if all columns exist\n",
        "    available_cols = [c for c in columns_to_keep if c in df.columns]\n",
        "    final_df = df[available_cols]\n",
        "\n",
        "    # Save to CSV\n",
        "    final_df.to_csv(output_path, index=False)\n",
        "    print(f\"‚úÖ Transformation complete.\")\n",
        "    print(f\"   Beta ratings calculated using N={N}.\")\n",
        "    print(f\"   File saved to: {output_path}\")\n",
        "\n",
        "# Run the transformation\n",
        "if __name__ == \"__main__\":\n",
        "    transform_ratings(INPUT_FILE, OUTPUT_FILE)"
      ],
      "metadata": {
        "id": "eefSYdt90CuQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}